from dotenv import load_dotenv
import os
from datetime import datetime
from zoneinfo import ZoneInfo
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, AIMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate
from tools import TOOLS
from faq_tool import get_faq_context
from operator import itemgetter
from langchain_core.runnables import RunnablePassthrough

# =====================================
# BASE
# =====================================

store = {}
TZ = ZoneInfo("America/Sao_Paulo")
today = datetime.now(TZ).date()

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

load_dotenv()

# =====================================
# MODELOS LLM
# =====================================

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    top_p=0.95,
    google_api_key=os.getenv("GEMINI_API_KEY")
)

llm_fast = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
    google_api_key=os.getenv("GEMINI_API_KEY")
)

# =====================================
# FUN√á√ÉO DE BADWORDS
# =====================================

def carregar_badwords(caminho="badwords.txt"):
    try:
        with open(caminho, "r", encoding="utf-8") as f:
            return [linha.strip().lower() for linha in f if linha.strip()]
    except FileNotFoundError:
        print(f"Aviso: Arquivo '{caminho}' n√£o encontrado. Nenhuma badword carregada.")
        return []

BAD_WORDS = carregar_badwords()

# =====================================
# PROMPTS
# =====================================

example_prompt_base = ChatPromptTemplate.from_messages([
    HumanMessagePromptTemplate.from_template("{human}"),
    AIMessagePromptTemplate.from_template("{ai}"),
])

PERSONA_SISTEMA_GAIA = """Voc√™ √© a Gaia ‚Äî uma assistente IA especialista em sustentabilidade e an√°lise de dados de carbono. Voc√™ √© objetiva, confi√°vel e emp√°tica, com foco em ajudar o usu√°rio a entender e reduzir seu impacto ambiental.
- Evite jarg√µes.
"""

# 1. Roteador

system_prompt_roteador = ("system",
    """
### PERSONA SISTEMA
{persona}

### PAPEL
- Acolher o usu√°rio e manter o foco em SUSTENTABILIDADE.
- Decidir a rota: {{diagnostico | carbono | faq}} ou se a pergunta √© fora escopo/sauda√ß√£o.
- Responder DIRETAMENTE em texto puro para:
  (a) `saudacao`: sauda√ß√µes/small talk.
  (b) `fora_de_escopo`: redirecionando para sustentabilidade.
- Usar o PROTOCOLO DE ENCAMINHAMENTO para:
  (a) `diagnostico`: An√°lise de dados, formul√°rios, crach√°s.
  (b) `carbono`: Dicas, conceitos de CO2, pegada de carbono (sem dados).
  (c) `faq`: D√∫vidas gerais sobre o sistema Gaia, o projeto, ou como funciona.

### REGRAS
- Para `saudacao` e `fora_de_escopo`, SEJA AMIG√ÅVEL e VARIE a resposta.
- Para `diagnostico`, `carbono` ou `faq`, N√ÉO responda ao usu√°rio, use o protocolo.

### PROTOCOLO DE ENCAMINHAMENTO (Texto puro)
ROUTE=<diagnostico|carbono|faq>
PERGUNTA_ORIGINAL=<mensagem completa do usu√°rio, sem edi√ß√µes>
PERSONA=<copie a PERSONA SISTEMA daqui>

### SA√çDAS POSS√çVEIS
- Resposta direta (texto curto) quando sauda√ß√£o ou fora de escopo.
- Encaminhamento ao especialista (diagnostico/carbono/faq) usando o protocolo.

### HIST√ìRICO DA CONVERSA
{chat_history}
"""
)

shots_roteador = [
    {
        "human": "Oi, tudo bem?",
        "ai": "Ol√°! Sou a Gaia üåø. Prontos para analisar alguns dados de sustentabilidade hoje?"
    },
    {
        "human": "Qual a previs√£o do tempo?",
        "ai": "Meu foco √© 100% em sustentabilidade. Posso ajudar analisando a m√©dia de emiss√£o da equipe ou dando dicas de CO2. O que prefere?"
    },
    {
        "human": "Qual a m√©dia de emiss√£o do time?",
        "ai": f"ROUTE=diagnostico\nPERGUNTA_ORIGINAL=Qual a m√©dia de emiss√£o do time?\nPERSONA={PERSONA_SISTEMA_GAIA}"
    },
    {
        "human": "O que √© pegada de carbono?",
        "ai": f"ROUTE=carbono\nPERGUNTA_ORIGINAL=O que √© pegada de carbono?\nPERSONA={PERSONA_SISTEMA_GAIA}"
    },
    {
        "human": "Me d√° os dados do crach√° 123.",
        "ai": f"ROUTE=diagnostico\nPERGUNTA_ORIGINAL=Me d√° os dados do crach√° 123.\nPERSONA={PERSONA_SISTEMA_GAIA}"
    },
    {
        "human": "O que √© o projeto Gaia?",
        "ai": f"ROUTE=faq\nPERGUNTA_ORIGINAL=O que √© o projeto Gaia?\nPERSONA={PERSONA_SISTEMA_GAIA}"
    },
    {
        "human": "Como meus dados s√£o usados pela Gaia?",
        "ai": f"ROUTE=faq\nPERGUNTA_ORIGINAL=Como meus dados s√£o usados pela Gaia?\nPERSONA={PERSONA_SISTEMA_GAIA}"
    },
    {
        "human": "Bom dia",
        "ai": "Bom dia! üí° Sobre o que vamos conversar hoje: an√°lise de dados ou dicas de sustentabilidade?"
    }
]

fewshots_roteador = FewShotChatMessagePromptTemplate(
    examples=shots_roteador,
    example_prompt=example_prompt_base
)

prompt_roteador = ChatPromptTemplate.from_messages([
    system_prompt_roteador,
    fewshots_roteador,
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
]).partial(persona=PERSONA_SISTEMA_GAIA)

# 2. Especialista de Carbono

system_prompt_carbono = ("system",
"""
### PAPEL
Voc√™ √© o "S√°bio da Sustentabilidade" da Gaia. Sua especialidade √© fornecer informa√ß√µes claras e factuais sobre sustentabilidade e emiss√µes de carbono. Sua base √© o conhecimento geral.

### CONTEXTO
- Voc√™ recebe uma `PERGUNTA_ORIGINAL` do roteador.
- Se a pergunta exigir uma an√°lise de dados, afirme que essa an√°lise deve ser feita pelo especialista de diagn√≥stico.

### REGRAS
- Sua resposta final DEVE ser um √∫nico e v√°lido objeto JSON.

### ESQUEMA DE SA√çDA (JSON OBRIGAT√ìRIO)
{{
  "dominio": "carbono",
  "intencao": "informar",
  "resposta": "Sua resposta clara e direta √† pergunta do usu√°rio.",
  "recomendacao": "Uma dica pr√°tica relacionada ao t√≥pico, ou uma string vazia se n√£o aplic√°vel."
}}

### HIST√ìRICO DA CONVERSA
{chat_history}
"""
)

shots_carbono = [
    {
        "human": "O que √© pegada de carbono?",
        "ai": """{
        "dominio": "carbono",
        "intencao": "informar",
        "resposta": "Pegada de carbono √© o volume total de gases de efeito estufa (GEE) gerados por nossas atividades di√°rias, medido em toneladas de CO2.",
        "recomendacao": "Pequenas a√ß√µes, como reduzir o consumo de carne ou usar menos o carro, ajudam a diminu√≠-la."
        }"""
    },
    {
        "human": "Como posso reduzir minha emiss√£o em casa?",
        "ai": """{
        "dominio": "carbono",
      "intencao": "informar",
        "resposta": "Em casa, o foco √© reduzir o consumo de energia e a produ√ß√£o de lixo.",
        "recomendacao": "Tente trocar l√¢mpadas comuns por LED, desligar aparelhos da tomada e separar seu lixo org√¢nico para compostagem."
        }"""
    },
    {
        "human": "Qual a m√©dia de emiss√£o da minha equipe?", 
        "ai": """{
        "dominio": "carbono",
        "intencao": "informar",
        "resposta": "Eu forne√ßo informa√ß√µes gerais sobre sustentabilidade. Para analisar dados espec√≠ficos da sua equipe, voc√™ precisa falar com nosso especialista em diagn√≥stico.",
        "recomendacao": "Tente perguntar 'qual a m√©dia de emiss√£o da empresa?' para o especialista correto."
        }"""
    }
]

fewshots_carbono = FewShotChatMessagePromptTemplate(
    examples=shots_carbono,
    example_prompt=example_prompt_base,
)

prompt_carbono = ChatPromptTemplate.from_messages([
    system_prompt_carbono,
    MessagesPlaceholder(variable_name="chat_history"),
    fewshots_carbono,
    ("human", "{input}"), 
])

# 3. Agente de Diagn√≥stico

system_prompt_diag = ("system",
"""
### PAPEL
Voc√™ √© um Analista de Dados S√™nior, especialista em sustentabilidade. Sua miss√£o √© transformar dados brutos de formul√°rios em insights acion√°veis usando as ferramentas dispon√≠veis.

### CONTEXTO
- Voc√™ recebe uma `PERGUNTA_ORIGINAL` do roteador.
- Hoje √© {today}

### FLUXO DE TRABALHO (ALGORITMO OBRIGAT√ìRIO)
1.  **An√°lise da Solicita√ß√£o**: Identifique se a pergunta √© sobre um indiv√≠duo (`numero_cracha`) ou coletivo.
2.  **Sele√ß√£o e Execu√ß√£o da Ferramenta**:
    - Para an√°lise individual, chame `query_formulario_funcionario`.
    - Para an√°lise geral/coletiva, chame `query_resumo_geral_formularios`.
3.  **An√°lise Cr√≠tica do Resultado**: Analise CUIDADOSAMENTE os dados retornados pela ferramenta.
4.  **Gera√ß√£o do JSON Final**: Com base na sua an√°lise, construa o objeto JSON de sa√≠da.

### MANUSEIO DE ERROS
- Se a ferramenta retornar um erro (ex: "Nenhum formul√°rio encontrado"), o campo `resposta` do seu JSON deve explicar o problema de forma amig√°vel.

### ESQUEMA DE SA√çDA (JSON OBRIGAT√ìRIO)
{{
  "dominio": "diagnostico",
  "intencao": "analisar",
  "resposta": "Sua an√°lise detalhada e conclus√µes baseadas nos dados da ferramenta. Em caso de erro, a explica√ß√£o amig√°vel do erro.",
  "recomendacao": "Uma sugest√£o pr√°tica e acion√°vel baseada nos dados. Em caso de erro, uma sugest√£o para resolver o problema."
}}

### HIST√ìRICO DA CONVERSA
{chat_history}
### HIST√ìRICO INTERNO DO AGENTE (N√£o mexa)
{agent_scratchpad}
"""
)


prompt_diag = ChatPromptTemplate.from_messages([
    system_prompt_diag, 
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
]).partial(today=today.isoformat())

# 4. Orquestrador

system_prompt_orq = ("system",
"""
### PAPEL
Voc√™ √© a "Voz de Gaia". Sua fun√ß√£o √© receber um objeto JSON t√©cnico de um especialista e traduzi-lo em uma mensagem final para o usu√°rio que seja calorosa, emp√°tica e motivadora.

### ENTRADA
Voc√™ receber√° um objeto JSON t√©cnico no campo `{input}`.

### REGRAS CR√çTICAS
- **Analise o `chat_history`. Se a conversa j√° come√ßou (ou seja, se `chat_history` n√£o estiver vazio), N√ÉO use sauda√ß√µes como "Ol√°!", "Oi!", etc. V√° direto ao ponto.**
- Sintetize a "resposta" do JSON em um par√°grafo amig√°vel.
- **Integre a "recomendacao" de forma fluida e natural no final da sua resposta, como uma dica ou sugest√£o.**
- **N√ÉO use o prefixo "Sugest√£o:" e N√ÉO pule uma linha.**
- Use emojis leves e apropriados (üåø, ‚ú®, üí°).
- N√ÉO invente informa√ß√µes. Baseie-se estritamente no JSON recebido.

### FORMATO DE SA√çDA
<Sua resposta amig√°vel e elaborada, j√° incluindo a recomenda√ß√£o no mesmo par√°grafo ou como continua√ß√£o direta.>

### HIST√ìRICO DA CONVERSA
{chat_history}
"""
)

shots_orquestrador = [
    {
        "human": """{
        "dominio": "diagnostico",
        "intencao": "analisar",
        "resposta": "An√°lise do crach√° 123: Emiss√£o de 80.5 kgCO2/m√™s. Principal fonte: Carro a Gasolina (40km/dia).",
        "recomendacao": "Considere usar o transporte fretado da empresa 2x por semana."
        }""",
        "ai": "Analisei os dados do crach√° 123 e a emiss√£o mensal √© de 80.5 kgCO2 üí°. A principal fonte √© o deslocamento com carro a gasolina, rodando cerca de 40km por dia. Que tal experimentar o transporte fretado da empresa duas vezes por semana? Isso j√° faria uma grande diferen√ßa! üåø"
    },
    {
        "human": """{
        "dominio": "carbono",
        "intencao": "informar",
        "resposta": "Pegada de carbono √© o volume total de gases de efeito estufa (GEE) gerados por nossas atividades.",
        "recomendacao": "Pequenas a√ß√µes, como reduzir o consumo de carne, ajudam a diminu√≠-la."
        }""",
        "ai": "A pegada de carbono √© basicamente o volume total de gases de efeito estufa que geramos em nossas atividades do dia a dia. ‚ú® Pequenas a√ß√µes j√° ajudam muito, como diminuir um pouco o consumo de carne vermelha durante a semana! üåø"
    },
    {
        "human": """{
        "dominio": "diagnostico",
        "intencao": "analisar",
        "resposta": "Erro: Nenhum formul√°rio encontrado para o crach√° 999.",
        "recomendacao": "Por favor, verifique se o n√∫mero do crach√° est√° correto."
        }""",
        "ai": "Hmm, parece que n√£o encontrei nenhum formul√°rio preenchido para o crach√° 999. Voc√™ poderia verificar se o n√∫mero est√° digitado corretamente, por favor? üí°"
    }
]

fewshots_orquestrador = FewShotChatMessagePromptTemplate(
    examples=shots_orquestrador,
    example_prompt=example_prompt_base,
)

prompt_orq = ChatPromptTemplate.from_messages([
    system_prompt_orq,
    MessagesPlaceholder(variable_name="chat_history"),
    fewshots_orquestrador,
    ("human", "{input}")
])

# 5. Juiz

system_prompt_juiz = ("system",
"""
### PAPEL
Voc√™ √© um Juiz de QA (Controle de Qualidade) de IA. Sua fun√ß√£o √© validar um objeto JSON gerado por um especialista.

### ENTRADA
1. A `PERGUNTA_ORIGINAL` do usu√°rio.
2. O `JSON_GERADO` pelo especialista.

### REGRAS DE VALIDA√á√ÉO
1.  **Relev√¢ncia**: O campo `resposta` no JSON responde DIRETAMENTE √† `PERGUNTA_ORIGINAL`?
2.  **Toxidade/Seguran√ßa**: O campo `resposta` ou `recomendacao` cont√©m linguagem ofensiva, perigosa, ilegal ou inapropriada?
3.  **Profissionalismo/Plausibilidade**: A resposta soa profissional? Ela n√£o parece inventada, exagerada ou uma alucina√ß√£o √≥bvia? (Ex: "Seu consumo √© o pior que j√° vi!").
4.  **Formato**: O `JSON_GERADO` √© um JSON sintaticamente v√°lido?

### SA√çDA (APENAS O C√ìDIGO)
- Se todas as regras passarem, responda APENAS: "APROVADO"
- Se QUALQUER regra falhar, responda APENAS com um dos seguintes c√≥digos:
  - "REPROVADO_RELEVANCIA": Se a regra 1 falhar.
  - "REPROVADO_TOXICIDADE": Se a regra 2 falhar.
  - "REPROVADO_ALUCINACAO": Se a regra 3 falhar (inventou fatos, n√£o profissional).
  - "REPROVADO_FORMATO": Se a regra 4 falhar (JSON quebrado ou inv√°lido).
""")

prompt_juiz = ChatPromptTemplate.from_messages([
    system_prompt_juiz,
    ("human", 
"""
### PERGUNTA_ORIGINAL
{pergunta}

### JSON_GERADO
{json_output}
"""
    )
])

# 6. FAQ

system_prompt_faq = ("system",
"""
### PAPEL
Voc√™ √© a Gaia, respondendo a perguntas gerais sobre o funcionamento do sistema, o projeto, e pol√≠ticas de dados, com base EXCLUSIVAMENTE no documento de FAQ fornecido.

### REGRAS
- Responda APENAS com base nos trechos de CONTEXTO fornecidos.
- Se a informa√ß√£o n√£o estiver no CONTEXTO, responda: "Desculpe, n√£o encontrei essa informa√ß√£o no nosso FAQ. üåø"
- Seja breve, clara e mantenha a persona emp√°tica da Gaia.
- N√£o invente informa√ß√µes.
- N√£o inclua sauda√ß√µes na resposta, v√° direto ao ponto.

### ENTRADA
- Voc√™ receber√° a pergunta do usu√°rio e o contexto do FAQ.
"""
)

prompt_faq = ChatPromptTemplate.from_messages([
    system_prompt_faq,
    ("human",
     "Pergunta do usu√°rio:\n{question}\n\n"
     "CONTEXTO (trechos do documento FAQ):\n{context}\n\n"
     "Responda com base APENAS no CONTEXTO.")
])

# =====================================
# AGENTES E CADEIAS
# =====================================

# Roteador
router_chain = RunnableWithMessageHistory(
    prompt_roteador | llm_fast | StrOutputParser(),
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# Especialista Carbono
carbono_chain = RunnableWithMessageHistory(
    prompt_carbono | llm_fast | StrOutputParser(),
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# Agente Diagn√≥stico
diag_agente = create_tool_calling_agent(llm, TOOLS, prompt_diag)
diag_exec = AgentExecutor(agent=diag_agente, tools=TOOLS, verbose=True)
diag_chain = RunnableWithMessageHistory(
    diag_exec,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# Orquestrador 
orquestrador_chain = RunnableWithMessageHistory(
    prompt_orq | llm_fast | StrOutputParser(),
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# Juiz
juiz_chain = (
    prompt_juiz
    | llm_fast 
    | StrOutputParser()
)

# FAQ
faq_chain = (
    RunnablePassthrough.assign(
        question=itemgetter("input"),
        context=lambda x: get_faq_context(x["input"])
    )
    | prompt_faq
    | llm_fast
    | StrOutputParser()
)

# =====================================
# EXECU√á√ÉO DO FLUXO
# =====================================

def executar_fluxo_gaia(pergunta_usuario: str, session_id: str):
    if any(word in pergunta_usuario.lower() for word in BAD_WORDS):
        return "Por favor, vamos manter a conversa respeitosa e focada em sustentabilidade. üåø"

    config = {"configurable": {"session_id": session_id}}
    chat_history = get_session_history(session_id)

    resposta_roteador = router_chain.invoke({"input": pergunta_usuario}, config=config).strip()

    resposta_final = "" 

    if not resposta_roteador.startswith("ROUTE="):
        resposta_final = resposta_roteador
    
    else:
        
        route_info = {}
        for line in resposta_roteador.split("\n"):
            if "=" in line:
                partes = line.split("=", 1)
                if len(partes) == 2:
                    route_info[partes[0].strip()] = partes[1].strip()

        route = route_info.get("ROUTE", "fora_de_escopo")

        especialista_input = route_info.get("PERGUNTA_ORIGINAL", pergunta_usuario)

        json_especialista = "" 
        
        if route == "carbono":
            json_especialista = carbono_chain.invoke(
                {"input": especialista_input},
                config=config
            )

        elif route == "diagnostico":
            try:
                resposta_agente = diag_chain.invoke(
                    {"input": especialista_input},
                    config=config
                )
                json_especialista = resposta_agente.get('output', str(resposta_agente))
            except Exception as e:
                json_especialista = '{ "dominio": "diagnostico", "intencao": "erro", "resposta": "Ocorreu um erro interno na ferramenta.", "recomendacao": "Tente novamente mais tarde." }'
            
        elif route == "faq":

            resposta_final = faq_chain.invoke(
                {"input": especialista_input}
            )

            if not resposta_final or not resposta_final.strip():
                 resposta_final = "Desculpe, n√£o encontrei essa informa√ß√£o no nosso FAQ. üåø"

        else:
            resposta_final = "Sou a Gaia e meu dever √© ajudar com sustentabilidade. Como posso te ajudar com isso? üåø"

        if json_especialista and not resposta_final:
            
            validacao_juiz = juiz_chain.invoke({
                "pergunta": especialista_input,
                "json_output": json_especialista
            }).strip()
            
            if validacao_juiz == "APROVADO":
                resposta_final = orquestrador_chain.invoke(
                    {"input": json_especialista},
                    config=config
                )
            else:
                
                if validacao_juiz == "REPROVADO_RELEVANCIA":
                    resposta_final = "Eu preparei uma resposta, mas notei que ela saiu um pouco do t√≥pico. Voc√™ poderia, por favor, reformular sua pergunta? üí°"
                elif validacao_juiz == "REPROVADO_ALUCINACAO":
                    resposta_final = "Hmm, n√£o consegui encontrar os dados exatos para sua solicita√ß√£o nos meus registros. Por favor, verifique sua pergunta (como o n√∫mero do crach√°) e tente novamente. üåø"
                elif validacao_juiz == "REPROVADO_TOXICIDADE":
                    resposta_final = "Ops! A resposta que eu ia te dar n√£o seguiu nossas diretrizes de comunidade. Por favor, tente perguntar de outra forma."
                elif validacao_juiz == "REPROVADO_FORMATO":
                    resposta_final = "Tive um problema t√©cnico ao gerar sua resposta (erro de formato). Por favor, tente novamente."
                else:
                    resposta_final = "N√£o consegui processar sua solicita√ß√£o com seguran√ßa no momento. Por favor, tente reformular sua pergunta. üí°"

    chat_history.add_user_message(pergunta_usuario)
    chat_history.add_ai_message(resposta_final)
    
    return resposta_final

# =====================================
# LOOP INTERATIVO
# =====================================

def run_local_chat():
    """Fun√ß√£o para executar o chat interativo no terminal."""
    SESSION_ID = "sessao_unica_local"
    while True:
        user_input = input("> ")
        if user_input.lower() in ["sair", "exit", "quit", "fim", "tchau"]:
            break
        try:
            resposta = executar_fluxo_gaia(user_input, session_id=SESSION_ID)
            print(f"\nGaia: {resposta}\n")
        except Exception as e:
            print(f"Ocorreu um erro inesperado: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    run_local_chat()