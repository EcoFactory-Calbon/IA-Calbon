# main.py

from dotenv import load_dotenv
import os
from datetime import datetime
from zoneinfo import ZoneInfo
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.agents import create_tool_calling_agent, AgentExecutor

# Importando sua lista de ferramentas do arquivo tools.py
from tools import TOOLS

# =====================================
# BASE
# =====================================
store = {}
TZ = ZoneInfo("America/Sao_Paulo")
today = datetime.now(TZ).date()

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

load_dotenv()

# =====================================
# MODELOS LLM
# =====================================
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    top_p=0.95,
    google_api_key=os.getenv("GEMINI_API_KEY")
)

llm_fast = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
    google_api_key=os.getenv("GEMINI_API_KEY")
)

# =====================================
# FUN√á√ÉO DE BADWORDS
# =====================================
def carregar_badwords(caminho="badwords.txt"):
    try:
        with open(caminho, "r", encoding="utf-8") as f:
            return [linha.strip().lower() for linha in f if linha.strip()]
    except FileNotFoundError:
        print(f"Aviso: Arquivo '{caminho}' n√£o encontrado. Nenhuma badword carregada.")
        return []

BAD_WORDS = carregar_badwords()

# =====================================
# PROMPTS DE N√çVEL PROFISSIONAL (COM CORRE√á√ÉO)
# =====================================

# 1. Roteador: O Porteiro Inteligente
system_prompt_roteador = """
### PAPEL
Voc√™ √© o "Porteiro de Inten√ß√µes" da Gaia. Sua √∫nica e exclusiva fun√ß√£o √© analisar a pergunta do usu√°rio e encaminh√°-la para o especialista correto, de forma r√°pida e precisa. Hoje √© {today}.

### CATEGORIAS DE TRIAGEM
1.  `diagnostico`: Solicita√ß√µes que exigem consulta a dados ou an√°lise. Exemplos: "Analise meu formul√°rio", "Qual a m√©dia da empresa?", "Quais os dados do crach√° 123?".
2.  `carbono`: Perguntas gerais sobre sustentabilidade, CO‚ÇÇ, ou funcionalidades do aplicativo que n√£o exigem acesso a dados. Exemplos: "O que √© pegada de carbono?", "Dicas para reduzir CO‚ÇÇ".
3.  `fora_de_escopo`: Qualquer outra coisa.

### FORMATO DE SA√çDA OBRIGAT√ìRIO
ROUTE=<diagnostico|carbono|fora_de_escopo>
PERGUNTA_ORIGINAL=<mensagem completa do usu√°rio>
"""
prompt_roteador = ChatPromptTemplate.from_messages([
    ("system", system_prompt_roteador),
    ("human", "{input}")
])

# 2. Especialista de Carbono: O S√°bio Focado
system_prompt_carbono = """
### PAPEL
Voc√™ √© o "S√°bio da Sustentabilidade" da Gaia. Sua especialidade √© fornecer informa√ß√µes claras e factuais sobre sustentabilidade e emiss√µes de carbono.

### DIRETRIZES
- Voc√™ N√ÉO tem acesso a dados de usu√°rios ou ferramentas. Sua base √© o conhecimento geral.
- Se a pergunta exigir uma an√°lise de dados, afirme que essa an√°lise deve ser feita pelo especialista de diagn√≥stico.
- Sua resposta final DEVE ser um √∫nico e v√°lido objeto JSON.

### ESQUEMA DE SA√çDA (JSON OBRIGAT√ìRIO)
{{
  "dominio": "carbono",
  "intencao": "informar",
  "resposta": "Sua resposta clara e direta √† pergunta do usu√°rio.",
  "recomendacao": "Uma dica pr√°tica relacionada ao t√≥pico, ou uma string vazia se n√£o aplic√°vel."
}}
""" # <<< CORRE√á√ÉO APLICADA AQUI com chaves duplas
prompt_carbono = ChatPromptTemplate.from_messages([
    ("system", system_prompt_carbono),
    ("human", "{input}"),
])

# 3. Agente de Diagn√≥stico: O Analista S√™nior
system_prompt_diag = """
### PAPEL
Voc√™ √© um Analista de Dados S√™nior, especialista em sustentabilidade. Sua miss√£o √© transformar dados brutos de formul√°rios em insights acion√°veis.

### FLUXO DE TRABALHO (ALGORITMO OBRIGAT√ìRIO)
1.  **An√°lise da Solicita√ß√£o**: Identifique se a pergunta do usu√°rio √© sobre um indiv√≠duo (requer `numero_cracha`) ou sobre o coletivo (resumo geral).
2.  **Sele√ß√£o e Execu√ß√£o da Ferramenta**:
    - Para an√°lise individual com crach√°, chame `query_formulario_funcionario`.
    - Para an√°lise geral/coletiva, chame `query_resumo_geral_formularios`.
3.  **An√°lise Cr√≠tica do Resultado**: Analise CUIDADOSAMENTE os dados retornados pela ferramenta.
4.  **Gera√ß√£o do JSON Final**: Com base na sua an√°lise, construa o objeto JSON de sa√≠da. Sua tarefa S√ì termina quando este JSON for gerado.

### MANUSEIO DE ERROS
- Se a ferramenta retornar um erro (ex: "Nenhum formul√°rio encontrado"), o campo `resposta` do seu JSON deve explicar o problema de forma amig√°vel e o campo `recomendacao` deve sugerir uma a√ß√£o (ex: "Por favor, verifique se o n√∫mero do crach√° est√° correto.").

### ESQUEMA DE SA√çDA (JSON OBRIGAT√ìRIO)
{{
  "dominio": "diagnostico",
  "intencao": "analisar",
  "resposta": "Sua an√°lise detalhada e conclus√µes baseadas nos dados da ferramenta. Em caso de erro, a explica√ß√£o amig√°vel do erro.",
  "recomendacao": "Uma sugest√£o pr√°tica e acion√°vel baseada nos dados. Em caso de erro, uma sugest√£o para resolver o problema."
}}
""" # <<< CORRE√á√ÉO APLICADA AQUI com chaves duplas
prompt_diag = ChatPromptTemplate.from_messages([
    ("system", system_prompt_diag),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

# 4. Orquestrador: A Voz Emp√°tica da Gaia
system_prompt_orq = """
### PAPEL
Voc√™ √© a "Voz de Gaia". Sua fun√ß√£o √© receber um objeto JSON t√©cnico de um especialista e traduzi-lo em uma mensagem final para o usu√°rio que seja calorosa, emp√°tica e motivadora.

### ENTRADA
Voc√™ receber√° um objeto JSON no campo `{input}` com os campos "resposta" e "recomendacao".

### REGRAS
- Sintetize a "resposta" em um par√°grafo amig√°vel.
- Apresente a "recomendacao" como uma "Sugest√£o" pr√°tica.
- Use emojis leves e apropriados (üåø, ‚ú®, üí°).
- N√ÉO invente informa√ß√µes. Baseie-se estritamente no JSON recebido.

### FORMATO DE SA√çDA
<Sua resposta amig√°vel e elaborada>

Sugest√£o: <Sua apresenta√ß√£o da recomenda√ß√£o de forma clara e motivadora>
"""
prompt_orq = ChatPromptTemplate.from_messages([
    ("system", system_prompt_orq),
    ("human", "{input}")
])

# =====================================
# AGENTES E CADEIAS
# =====================================
carbono_chain = prompt_carbono | llm_fast | StrOutputParser()

diag_agente = create_tool_calling_agent(llm, TOOLS, prompt_diag)
diag_exec = AgentExecutor(agent=diag_agente, tools=TOOLS, verbose=True)
diag_chain = RunnableWithMessageHistory(
    diag_exec,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

router_chain = prompt_roteador | llm_fast | StrOutputParser()
orquestrador_chain = prompt_orq | llm_fast | StrOutputParser()

# =====================================
# EXECU√á√ÉO DO FLUXO
# =====================================
def executar_fluxo_gaia(pergunta_usuario: str, session_id: str):
    if any(word in pergunta_usuario.lower() for word in BAD_WORDS):
        return "Por favor, vamos manter a conversa respeitosa e focada em sustentabilidade. üåø"

    chat_history = get_session_history(session_id)

    resposta_roteador = router_chain.invoke({
        "input": pergunta_usuario,
        "today": today.isoformat()
    }).strip()
    print(f"[DEBUG] Roteador retornou:\n{resposta_roteador}\n")

    route_info = {line.split("=", 1)[0].strip(): line.split("=", 1)[1].strip()
                  for line in resposta_roteador.split("\n") if "=" in line}
    route = route_info.get("ROUTE", "fora_de_escopo")
    especialista_input = route_info.get("PERGUNTA_ORIGINAL", pergunta_usuario)

    resposta_final = ""
    if route == "carbono":
        print(f"[DEBUG] Chamando especialista CARBONO...")
        json_especialista = carbono_chain.invoke({"input": especialista_input})
        print(f"[DEBUG] Especialista CARBONO respondeu:\n{json_especialista}")
        resposta_final = orquestrador_chain.invoke({"input": json_especialista})

    elif route == "diagnostico":
        print(f"[DEBUG] Chamando AGENTE DE DIAGN√ìSTICO...")
        resposta_agente = diag_chain.invoke(
            {"input": especialista_input},
            config={"configurable": {"session_id": session_id}}
        )
        json_analisado = resposta_agente['output']
        print(f"[DEBUG] Agente DIAGN√ìSTICO respondeu:\n{json_analisado}")
        resposta_final = orquestrador_chain.invoke({"input": json_analisado})

    else:
        resposta_final = "Ol√°! Sou a Gaia. Meu foco √© ajudar com a redu√ß√£o de emiss√µes de carbono. Como posso te apoiar nesse tema hoje? üåø"

    chat_history.add_user_message(pergunta_usuario)
    chat_history.add_ai_message(resposta_final)
    return resposta_final

# =====================================
# LOOP INTERATIVO
# =====================================
print("üåø Gaia iniciada (vers√£o profissional). Diga 'sair' para encerrar.\n")
SESSION_ID = "sessao_unica"

while True:
    user_input = input("> ")
    if user_input.lower() in ["sair", "exit", "quit", "fim", "tchau"]:
        print("\nGaia: At√© logo! üåø")
        break
    try:
        resposta = executar_fluxo_gaia(user_input, session_id=SESSION_ID)
        print(f"\nGaia: {resposta}\n")
    except Exception as e:
        print(f"Ocorreu um erro inesperado: {e}")