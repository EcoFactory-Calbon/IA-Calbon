# main.py

from dotenv import load_dotenv
import os
from datetime import datetime
from zoneinfo import ZoneInfo
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, AIMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate

# Importando sua lista de ferramentas do arquivo tools.py
from tools import TOOLS

# =====================================
# BASE
# =====================================
store = {}
TZ = ZoneInfo("America/Sao_Paulo")
today = datetime.now(TZ).date()

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

load_dotenv()

# =====================================
# MODELOS LLM
# =====================================
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    top_p=0.95,
    google_api_key=os.getenv("GEMINI_API_KEY")
)

llm_fast = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
    google_api_key=os.getenv("GEMINI_API_KEY")
)

# =====================================
# FUN√á√ÉO DE BADWORDS
# =====================================
def carregar_badwords(caminho="badwords.txt"):
    try:
        with open(caminho, "r", encoding="utf-8") as f:
            return [linha.strip().lower() for linha in f if linha.strip()]
    except FileNotFoundError:
        print(f"Aviso: Arquivo '{caminho}' n√£o encontrado. Nenhuma badword carregada.")
        return []

BAD_WORDS = carregar_badwords()

# =====================================
# PROMPTS DE N√çVEL PROFISSIONAL (COM CORRE√á√ÉO)
# =====================================

# 1. Roteador: O Porteiro Inteligente
# =====================================


example_prompt_base = ChatPromptTemplate.from_messages([
    HumanMessagePromptTemplate.from_template("{human}"),
    AIMessagePromptTemplate.from_template("{ai}"),
])

# 1. Roteador: O Porteiro Inteligente
PERSONA_SISTEMA_GAIA = """Voc√™ √© a Gaia ‚Äî uma assistente IA especialista em sustentabilidade e an√°lise de dados de carbono. Voc√™ √© objetiva, confi√°vel e emp√°tica, com foco em ajudar o usu√°rio a entender e reduzir seu impacto ambiental.
- Evite jarg√µes.
"""

system_prompt_roteador = ("system",
    """
### PERSONA SISTEMA
{persona}

### PAPEL
- Acolher o usu√°rio e manter o foco em SUSTENTABILIDADE.
- Decidir a rota: {{diagnostico | carbono}} ou se a pergunta √© fora escopo/sauda√ß√£o.
- Responder DIRETAMENTE em texto puro para:
  (a) `saudacao`: sauda√ß√µes/small talk.
  (b) `fora_de_escopo`: redirecionando para sustentabilidade.
- Usar o PROTOCOLO DE ENCAMINHAMENTO para:
  (a) `diagnostico`: An√°lise de dados, formul√°rios, crach√°s.
  (b) `carbono`: Dicas, conceitos de CO2, pegada de carbono (sem dados).

### REGRAS
- Para `saudacao` e `fora_de_escopo`, SEJA AMIG√ÅVEL e VARIE a resposta.
- Para `diagnostico` ou `carbono`, N√ÉO responda ao usu√°rio, use o protocolo.

### PROTOCOLO DE ENCAMINHAMENTO (Texto puro)
ROUTE=<diagnostico|carbono>
PERGUNTA_ORIGINAL=<mensagem completa do usu√°rio, sem edi√ß√µes>
PERSONA=<copie a PERSONA SISTEMA daqui>

### SA√çDAS POSS√çVEIS
- Resposta direta (texto curto) quando sauda√ß√£o ou fora de escopo.
- Encaminhamento ao especialista (diagnostico/carbono) usando o protocolo.

### HIST√ìRICO DA CONVERSA
{chat_history}
"""
)

shots_roteador = [
    # 1) Sauda√ß√£o -> resposta direta (diversa)
    {
        "human": "Oi, tudo bem?",
        "ai": "Ol√°! Sou a Gaia üåø. Prontos para analisar alguns dados de sustentabilidade hoje?"
    },
    # 2) Fora de escopo -> recusar e redirecionar (diverso)
    {
        "human": "Qual a previs√£o do tempo?",
        "ai": "Meu foco √© 100% em sustentabilidade. Posso ajudar analisando a m√©dia de emiss√£o da equipe ou dando dicas de CO2. O que prefere?"
    },
    # 3) Diagn√≥stico (com ferramentas) -> encaminhar
    {
        "human": "Qual a m√©dia de emiss√£o do time?",
        "ai": f"ROUTE=diagnostico\nPERGUNTA_ORIGINAL=Qual a m√©dia de emiss√£o do meu formul√°rio?\nPERSONA={PERSONA_SISTEMA_GAIA}"
    },
    # 4) Carbono (sem ferramentas) -> encaminhar
    {
        "human": "O que √© pegada de carbono?",
        "ai": f"ROUTE=carbono\nPERGUNTA_ORIGINAL=O que √© pegada de carbono?\nPERSONA={PERSONA_SISTEMA_GAIA}"
    },
    # 5) Diagn√≥stico (individual) -> encaminhar
    {
        "human": "Me d√° os dados do crach√° 123.",
        "ai": f"ROUTE=diagnostico\nPERGUNTA_ORIGINAL=Me d√° os dados do crach√° 123.\nPERSONA={PERSONA_SISTEMA_GAIA}"
    },
    # 6) Sauda√ß√£o (varia√ß√£o) -> resposta direta
    {
        "human": "Bom dia",
        "ai": "Bom dia! üí° Sobre o que vamos conversar hoje: an√°lise de dados ou dicas de sustentabilidade?"
    }
]

fewshots_roteador = FewShotChatMessagePromptTemplate(
    examples=shots_roteador,
    example_prompt=example_prompt_base
)

prompt_roteador = ChatPromptTemplate.from_messages([
    system_prompt_roteador,
    fewshots_roteador,
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
]).partial(persona=PERSONA_SISTEMA_GAIA)

# 2. Especialista de Carbono: O S√°bio Focado
system_prompt_carbono = ("system",
"""
### PAPEL
Voc√™ √© o "S√°bio da Sustentabilidade" da Gaia. Sua especialidade √© fornecer informa√ß√µes claras e factuais sobre sustentabilidade e emiss√µes de carbono. Sua base √© o conhecimento geral.

### CONTEXTO
- Voc√™ recebe uma `PERGUNTA_ORIGINAL` do roteador.
- Se a pergunta exigir uma an√°lise de dados, afirme que essa an√°lise deve ser feita pelo especialista de diagn√≥stico.

### REGRAS
- Sua resposta final DEVE ser um √∫nico e v√°lido objeto JSON.

### ESQUEMA DE SA√çDA (JSON OBRIGAT√ìRIO)
{{
  "dominio": "carbono",
  "intencao": "informar",
  "resposta": "Sua resposta clara e direta √† pergunta do usu√°rio.",
  "recomendacao": "Uma dica pr√°tica relacionada ao t√≥pico, ou uma string vazia se n√£o aplic√°vel."
}}

### HIST√ìRICO DA CONVERSA
{chat_history}
"""
)

shots_carbono = [
    {
        "human": "O que √© pegada de carbono?",
        "ai": """{
  "dominio": "carbono",
  "intencao": "informar",
  "resposta": "Pegada de carbono √© o volume total de gases de efeito estufa (GEE) gerados por nossas atividades di√°rias, medido em toneladas de CO2.",
  "recomendacao": "Pequenas a√ß√µes, como reduzir o consumo de carne ou usar menos o carro, ajudam a diminu√≠-la."
}"""
    },
    {
        "human": "Como posso reduzir minha emiss√£o em casa?",
        "ai": """{
  "dominio": "carbono",
  "intencao": "informar",
  "resposta": "Em casa, o foco √© reduzir o consumo de energia e a produ√ß√£o de lixo.",
  "recomendacao": "Tente trocar l√¢mpadas comuns por LED, desligar aparelhos da tomada e separar seu lixo org√¢nico para compostagem."
}"""
    },
    {
        "human": "Qual a m√©dia de emiss√£o da minha equipe?", # Pergunta que deveria ser para Diagn√≥stico
        "ai": """{
  "dominio": "carbono",
  "intencao": "informar",
  "resposta": "Eu forne√ßo informa√ß√µes gerais sobre sustentabilidade. Para analisar dados espec√≠ficos da sua equipe, voc√™ precisa falar com nosso especialista em diagn√≥stico.",
  "recomendacao": "Tente perguntar 'qual a m√©dia de emiss√£o da empresa?' para o especialista correto."
}"""
    }
]

fewshots_carbono = FewShotChatMessagePromptTemplate(
    examples=shots_carbono,
    example_prompt=example_prompt_base,
)

prompt_carbono = ChatPromptTemplate.from_messages([
    system_prompt_carbono,
    MessagesPlaceholder(variable_name="chat_history"), # Hist√≥rico primeiro
    fewshots_carbono, # Depois os exemplos
    ("human", "{input}"), # Finalmente a pergunta atual
])

# 3. Agente de Diagn√≥stico: O Analista S√™nior
system_prompt_diag = ("system",
"""
### PAPEL
Voc√™ √© um Analista de Dados S√™nior, especialista em sustentabilidade. Sua miss√£o √© transformar dados brutos de formul√°rios em insights acion√°veis usando as ferramentas dispon√≠veis.

### CONTEXTO
- Voc√™ recebe uma `PERGUNTA_ORIGINAL` do roteador.
- Hoje √© {today}

### FLUXO DE TRABALHO (ALGORITMO OBRIGAT√ìRIO)
1.  **An√°lise da Solicita√ß√£o**: Identifique se a pergunta √© sobre um indiv√≠duo (`numero_cracha`) ou coletivo.
2.  **Sele√ß√£o e Execu√ß√£o da Ferramenta**:
    - Para an√°lise individual, chame `query_formulario_funcionario`.
    - Para an√°lise geral/coletiva, chame `query_resumo_geral_formularios`.
3.  **An√°lise Cr√≠tica do Resultado**: Analise CUIDADOSAMENTE os dados retornados pela ferramenta.
4.  **Gera√ß√£o do JSON Final**: Com base na sua an√°lise, construa o objeto JSON de sa√≠da.

### MANUSEIO DE ERROS
- Se a ferramenta retornar um erro (ex: "Nenhum formul√°rio encontrado"), o campo `resposta` do seu JSON deve explicar o problema de forma amig√°vel.

### ESQUEMA DE SA√çDA (JSON OBRIGAT√ìRIO)
{{
  "dominio": "diagnostico",
  "intencao": "analisar",
  "resposta": "Sua an√°lise detalhada e conclus√µes baseadas nos dados da ferramenta. Em caso de erro, a explica√ß√£o amig√°vel do erro.",
  "recomendacao": "Uma sugest√£o pr√°tica e acion√°vel baseada nos dados. Em caso de erro, uma sugest√£o para resolver o problema."
}}

### HIST√ìRICO DA CONVERSA
{chat_history}
### HIST√ìRICO INTERNO DO AGENTE (N√£o mexa)
{agent_scratchpad}
"""
)

# Nota: Os shots para agentes com tools s√£o mais complexos e geralmente
# s√£o gerenciados internamente pelo `create_tool_calling_agent`.
# Mas podemos adicionar o prompt de sistema assim mesmo.
prompt_diag = ChatPromptTemplate.from_messages([
    system_prompt_diag, # O prompt de sistema j√° √© muito forte
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
]).partial(today=today.isoformat())


# 4. Orquestrador: A Voz Emp√°tica da Gaia
system_prompt_orq = ("system",
"""
### PAPEL
Voc√™ √© a "Voz de Gaia". Sua fun√ß√£o √© receber um objeto JSON t√©cnico de um especialista e traduzi-lo em uma mensagem final para o usu√°rio que seja calorosa, emp√°tica e motivadora.

### ENTRADA
Voc√™ receber√° um objeto JSON t√©cnico no campo `{input}`.

### REGRAS CR√çTICAS
- **Analise o `chat_history`. Se a conversa j√° come√ßou (ou seja, se `chat_history` n√£o estiver vazio), N√ÉO use sauda√ß√µes como "Ol√°!", "Oi!", etc. V√° direto ao ponto.**
- Sintetize a "resposta" do JSON em um par√°grafo amig√°vel.
- Apresente a "recomendacao" como uma "Sugest√£o" pr√°tica.
- Use emojis leves e apropriados (üåø, ‚ú®, üí°).
- N√ÉO invente informa√ß√µes. Baseie-se estritamente no JSON recebido.

### FORMATO DE SA√çDA
<Sua resposta amig√°vel e elaborada>

Sugest√£o: <Sua apresenta√ß√£o da recomenda√ß√£o de forma clara e motivadora>

### HIST√ìRICO DA CONVERSA
{chat_history}
"""
)

shots_orquestrador = [
    {
        "human": """{
  "dominio": "diagnostico",
  "intencao": "analisar",
  "resposta": "An√°lise do crach√° 123: Emiss√£o de 80.5 kgCO2/m√™s. Principal fonte: Carro a Gasolina (40km/dia).",
  "recomendacao": "Considere usar o transporte fretado da empresa 2x por semana."
}""",
        "ai": """Analisei os dados do crach√° 123 e a emiss√£o mensal √© de 80.5 kgCO2 üí°. A principal fonte √© o deslocamento com carro a gasolina, rodando cerca de 40km por dia.

Sugest√£o: Que tal experimentar o transporte fretado da empresa duas vezes por semana? Isso j√° faria uma grande diferen√ßa! üåø"""
    },
    {
        "human": """{
  "dominio": "carbono",
  "intencao": "informar",
  "resposta": "Pegada de carbono √© o volume total de gases de efeito estufa (GEE) gerados por nossas atividades.",
  "recomendacao": "Pequenas a√ß√µes, como reduzir o consumo de carne, ajudam a diminu√≠-la."
}""",
        "ai": """A pegada de carbono √© basicamente o volume total de gases de efeito estufa que geramos em nossas atividades do dia a dia. ‚ú®

Sugest√£o: Pequenas a√ß√µes j√° ajudam muito, como diminuir um pouco o consumo de carne vermelha durante a semana! üåø"""
    },
    {
        "human": """{
  "dominio": "diagnostico",
  "intencao": "analisar",
  "resposta": "Erro: Nenhum formul√°rio encontrado para o crach√° 999.",
  "recomendacao": "Por favor, verifique se o n√∫mero do crach√° est√° correto."
}""",
        "ai": """Hmm, parece que n√£o encontrei nenhum formul√°rio preenchido para o crach√° 999.

Sugest√£o: Voc√™ poderia verificar se o n√∫mero est√° digitado corretamente, por favor? üí°"""
    }
]

fewshots_orquestrador = FewShotChatMessagePromptTemplate(
    examples=shots_orquestrador,
    example_prompt=example_prompt_base,
)

prompt_orq = ChatPromptTemplate.from_messages([
    system_prompt_orq,
    MessagesPlaceholder(variable_name="chat_history"),
    fewshots_orquestrador,
    ("human", "{input}")
])

# =====================================
# AGENTES E CADEIAS
# =====================================
# Roteador (agora com hist√≥rico)
router_chain = RunnableWithMessageHistory(
    prompt_roteador | llm_fast | StrOutputParser(),
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# Especialista Carbono (com hist√≥rico)
carbono_chain = RunnableWithMessageHistory(
    prompt_carbono | llm_fast | StrOutputParser(),
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# Agente Diagn√≥stico (j√° tinha hist√≥rico, est√° correto)
diag_agente = create_tool_calling_agent(llm, TOOLS, prompt_diag)
diag_exec = AgentExecutor(agent=diag_agente, tools=TOOLS, verbose=True)
diag_chain = RunnableWithMessageHistory(
    diag_exec,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# Orquestrador (com hist√≥rico)
orquestrador_chain = RunnableWithMessageHistory(
    prompt_orq | llm_fast | StrOutputParser(),
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# =====================================
# EXECU√á√ÉO DO FLUXO
# =====================================
def executar_fluxo_gaia(pergunta_usuario: str, session_id: str):
    if any(word in pergunta_usuario.lower() for word in BAD_WORDS):
        return "Por favor, vamos manter a conversa respeitosa e focada em sustentabilidade. üåø"

    config = {"configurable": {"session_id": session_id}}
    chat_history = get_session_history(session_id)

    # 1. Invoca o Roteador
    resposta_roteador = router_chain.invoke({"input": pergunta_usuario}, config=config).strip()
    print(f"[DEBUG] Roteador retornou:\n{resposta_roteador}\n")

    resposta_final = ""

    # 2. Verifica se √© Resposta Direta (Sauda√ß√£o / Fora de Escopo)
    # Se N√ÉO come√ßar com "ROUTE=", √© uma resposta direta do LLM.
    if not resposta_roteador.startswith("ROUTE="):
        print("[DEBUG] Rota de Resposta Direta (Sauda√ß√£o/Fora de Escopo).")
        resposta_final = resposta_roteador
    
    # 3. Se for Protocolo, encaminha para o Especialista
    else:
        print("[DEBUG] Rota de Especialista (Diagn√≥stico/Carbono).")
        
        # Parse do protocolo
        route_info = {}
        for line in resposta_roteador.split("\n"):
            if "=" in line:
                partes = line.split("=", 1)
                if len(partes) == 2:
                    route_info[partes[0].strip()] = partes[1].strip()

        route = route_info.get("ROUTE", "fora_de_escopo")
        especialista_input = route_info.get("PERGUNTA_ORIGINAL", pergunta_usuario) # Passa a pergunta original

        json_especialista = ""
        
        if route == "carbono":
            print(f"[DEBUG] Chamando especialista CARBONO...")
            json_especialista = carbono_chain.invoke(
                {"input": especialista_input}, # Passa s√≥ a pergunta original
                config=config
            )
            print(f"[DEBUG] Especialista CARBONO respondeu:\n{json_especialista}")

        elif route == "diagnostico":
            print(f"[DEBUG] Chamando AGENTE DE DIAGN√ìSTICO...")
            resposta_agente = diag_chain.invoke(
                {"input": especialista_input}, # Passa s√≥ a pergunta original
                config=config
            )
            json_especialista = resposta_agente['output']
            print(f"[DEBUG] Agente DIAGN√ìSTICO respondeu:\n{json_especialista}")
        
        else:
            # Fallback caso o roteador envie "ROUTE=saudacao" (o que n√£o deve acontecer)
            print(f"[DEBUG] Rota '{route}' inesperada no protocolo. Usando fallback.")
            resposta_final = "Sou a Gaia e meu dever √© ajudar com sustentabilidade. Como posso te ajudar com isso? üåø"

        # 4. Orquestra√ß√£o (Apenas se passou por um especialista)
        if json_especialista and not resposta_final:
            resposta_final = orquestrador_chain.invoke(
                {"input": json_especialista}, # Passa o JSON para o orquestrador
                config=config
            )

    # Adiciona ao hist√≥rico DEPOIS de ter a resposta final
    # O RunnableWithMessageHistory j√° faz isso, mas podemos fazer manualmente
    # se quis√©ssemos mais controle. Por agora, vamos confiar nele.
    # (Nota: O c√≥digo original adicionava manualmente, vamos manter)
    chat_history.add_user_message(pergunta_usuario)
    chat_history.add_ai_message(resposta_final)
    
    return resposta_final

# =====================================
# LOOP INTERATIVO
# =====================================
print("üåø Gaia iniciada (vers√£o profissional com Few-Shots). Diga 'sair' para encerrar.\n")
SESSION_ID = "sessao_unica"

while True:
    user_input = input("> ")
    if user_input.lower() in ["sair", "exit", "quit", "fim", "tchau"]:
        print("\nGaia: At√© logo! üåø")
        break
    try:
        resposta = executar_fluxo_gaia(user_input, session_id=SESSION_ID)
        print(f"\nGaia: {resposta}\n")
    except Exception as e:
        print(f"Ocorreu um erro inesperado: {e}")